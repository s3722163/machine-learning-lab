{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div align=\"center\"><font color='green'> COSC 2673/2793 | Machine Learning  </font></div>\n",
    "## <div align=\"center\"> <font color='green'> Week 6 Lab Exercises: **Decision Trees**</font></div>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "During the last couple of weeks we learned about the typical ML model development process. In this weeks lab we will explore decision tree based models. \n",
    "\n",
    "The lab assumes that you have completed the labs for week 2-5. If you havent yet, please do so before attempting this lab. \n",
    "\n",
    "The lab can be executed on either your own machine (with anaconda installation) or on AWS educate classroom setup for the course. \n",
    "- Please refer canvas for instructions on installing anaconda python or setting up AWS Sagemaker notebook: [Introduction to Amazon Web Services (AWS) Classrooms](https://rmit.instructure.com/courses/79534/pages/introduction-to-amazon-web-services-aws-classrooms?module_item_id=2952364)\n",
    "\n",
    "\n",
    "## Objective\n",
    "- Continue to familiarise with Python and other ML packages.\n",
    "- Learning classification decision trees from both categorical and continuous numerical data\n",
    "- Comparing the performance of various trees after pruned.\n",
    "- Learning regression decision trees and comparing these models to regression models from previous labs.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. \n",
    "\n",
    "#### Input variables:\n",
    "- Bank client data:\n",
    "    1. age (numeric)\n",
    "    2. job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\", \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \n",
    "    3. marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n",
    "    4. education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
    "    5. default: has credit in default? (binary: \"yes\",\"no\")\n",
    "    6. balance: average yearly balance, in euros (numeric) \n",
    "    7. housing: has housing loan? (binary: \"yes\",\"no\")\n",
    "    8. loan: has personal loan? (binary: \"yes\",\"no\")\n",
    "- Related with the last contact of the current campaign:\n",
    "    9. contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \n",
    "    10. day: last contact day of the month (numeric)\n",
    "    11. month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "    12. duration: last contact duration, in seconds (numeric)\n",
    "- Other attributes:\n",
    "    13. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "    14. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
    "    15. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "    16. poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n",
    "    \n",
    "#### Output variable (desired target):   \n",
    "    17. y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
    "    \n",
    "This dataset is public available for research. The details are described in Moro et al., 2011.\n",
    "\n",
    "Moro et al., 2011: S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \n",
    "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. \n",
    "\n",
    "Lets read the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./bank-full.csv', delimiter=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains categorical and numerical attributes. Lets convert the categorical columns to categorical data type in pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype == object:\n",
    "        data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn’s classification decision tree learner doesn’t work with categorical attributes.  It only works with continuous numeric attributes.  The target class, however, must be categorical.  So the categorical attributed must be converted into a suitable continuous format. Helpfully, Pandas can do this. \n",
    "\n",
    "First, split the data into the target class and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         no\n",
      "1         no\n",
      "2         no\n",
      "3         no\n",
      "4         no\n",
      "        ... \n",
      "45206    yes\n",
      "45207    yes\n",
      "45208    yes\n",
      "45209     no\n",
      "45210     no\n",
      "Name: y, Length: 45211, dtype: category\n",
      "Categories (2, object): ['no', 'yes']\n"
     ]
    }
   ],
   "source": [
    "dataY = data['y']\n",
    "dataX = data.drop(columns='y')\n",
    "print(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use Pandas to generate \"numerical\" versions of the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>job_admin.</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>2143</td>\n",
       "      <td>5</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1506</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  balance  day  duration  campaign  pdays  previous  job_admin.  \\\n",
       "0   58     2143    5       261         1     -1         0           0   \n",
       "1   44       29    5       151         1     -1         0           0   \n",
       "2   33        2    5        76         1     -1         0           0   \n",
       "3   47     1506    5        92         1     -1         0           0   \n",
       "4   33        1    5       198         1     -1         0           0   \n",
       "\n",
       "   job_blue-collar  job_entrepreneur  ...  month_jun  month_mar  month_may  \\\n",
       "0                0                 0  ...          0          0          1   \n",
       "1                0                 0  ...          0          0          1   \n",
       "2                0                 1  ...          0          0          1   \n",
       "3                1                 0  ...          0          0          1   \n",
       "4                0                 0  ...          0          0          1   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_failure  poutcome_other  \\\n",
       "0          0          0          0                 0               0   \n",
       "1          0          0          0                 0               0   \n",
       "2          0          0          0                 0               0   \n",
       "3          0          0          0                 0               0   \n",
       "4          0          0          0                 0               0   \n",
       "\n",
       "   poutcome_success  poutcome_unknown  \n",
       "0                 0                 1  \n",
       "1                 0                 1  \n",
       "2                 0                 1  \n",
       "3                 0                 1  \n",
       "4                 0                 1  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataXExpand = pd.get_dummies(dataX)\n",
    "dataXExpand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the categories are expanded into boolean (yes/no, that is, 1/0) values that can be treated as continuous numerical values. It’s not ideal, but it will allow a correct decision tree to be learned.\n",
    "\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Why is it necessary to convert the attributes into boolean representations, rather than just convert them into integer values? What problem would be caused by converting the attributes into integers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target class also needs to be pre-processed.  The target will be treated by sklearn as a category, but sklearn requires that these categories are represented as integers (not strings). To convert the strings into numbers, the preprocessing. LabelEncoder class from sklearn can be used, as shown below. The two print statements show how to convert in both directions (strings to integers, and vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n",
      "['no' 'yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dataY)\n",
    "class_labels = le.inverse_transform([0,1])\n",
    "dataY = le.transform(dataY)\n",
    "print(dataY)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Since we have covered how to do EDA in the previous labs, this section is left as an exercise for you. Complete the EDA and use the information to justify the decisions made in the subsequent code blocks.</font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the performance (evaluation) metric\n",
    "There are many performance metrics that apply to this problem such as `accuracy_score`, `f1_score`, etc. More information on performance metrics available in sklearn can be found at: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "The insights gained in the EDA becomes vital in determining the performance metric. Try to identify the characteristics that are important in making this decision from the EDA results. Use your judgment to pick the best performance measure - discuss with the lab demonstrator to see if the performance measure you came up with is appropriate. \n",
    "\n",
    "\n",
    "In this task, I want to give equal importance to all classes. Therefore I will select `macro-averaged` `f1_score` as my performance measure and I wish to achieve a target value of 75% f1_score. \n",
    "\n",
    "F1-score is NOT the only performance measure that can be used for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the experiment - data splits\n",
    "\n",
    "Next **what data should we use to evaluate the performance?**\n",
    "\n",
    "\n",
    "We can generate \"simulated\" unseen data in several methods\n",
    "1. Hold-Out validation\n",
    "2. Cross-Validation\n",
    "\n",
    "Lets use hold out validation for this experiment.\n",
    "\n",
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Use the knowledge from last couple of weeks to split the data appropriately.</font>**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27126, 51) (9042, 51) (9043, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_X_, test_data_X, train_data_y_ , test_data_y = train_test_split(dataXExpand, dataY, test_size=0.2, \n",
    "                                              shuffle=True,random_state=0)\n",
    "    \n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_X, val_data_X, train_data_y, val_data_y = train_test_split(train_data_X_, train_data_y_, test_size=0.25, \n",
    "                                            shuffle=True,random_state=0)\n",
    "    \n",
    "print(train_data_X.shape, val_data_X.shape, test_data_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data_X.to_numpy()\n",
    "train_y = train_data_y\n",
    "\n",
    "test_X = test_data_X.to_numpy()\n",
    "test_y = test_data_y\n",
    "\n",
    "val_X = val_data_X.to_numpy()\n",
    "val_y = val_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup few functions to visualise the results.\n",
    "\n",
    "(Ignore section if on AWS) It is likely that you won’t have the graphviz package available, in which case you will need to install graphviz. This can be done through the anacoda-navigator interface (environment tab):\n",
    "1. Change the dropbox to “All”\n",
    "2. Search for the packagecpython-graphviz\n",
    "3. Select the python-graphviz package and install (press “apply”)\n",
    "\n",
    "If you cant install graphviz don’t worry - you can still complete the lab. Graphviz is nice to be able to see the trees that are being calculated. However, once the trees become complex, visualising them isn’t practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "\n",
    "def get_tree_2_plot(clf):\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=dataXExpand.columns,  \n",
    "                      class_names=class_labels,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_acc_scores(clf, train_X, train_y, val_X, val_y):\n",
    "    train_pred = clf.predict(train_X)\n",
    "    val_pred = clf.predict(val_X)\n",
    "    \n",
    "    train_acc = f1_score(train_y, train_pred, average='macro')\n",
    "    val_acc = f1_score(val_y, val_pred, average='macro')\n",
    "    \n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple decision tree training\n",
    "\n",
    "Lets train a simple decision tree and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_max_depth = 3   #change this value and observe\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=tree_max_depth, class_weight='balanced')\n",
    "clf = clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.1 (20210222.2046)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"1884pt\" height=\"433pt\"\n",
       " viewBox=\"0.00 0.00 1883.50 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 1879.5,-429 1879.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M1006,-425C1006,-425 817,-425 817,-425 811,-425 805,-419 805,-413 805,-413 805,-354 805,-354 805,-348 811,-342 817,-342 817,-342 1006,-342 1006,-342 1012,-342 1018,-348 1018,-354 1018,-354 1018,-413 1018,-413 1018,-419 1012,-425 1006,-425\"/>\n",
       "<text text-anchor=\"start\" x=\"850.5\" y=\"-409.8\" font-family=\"helvetica\" font-size=\"14.00\">duration ≤ 205.5</text>\n",
       "<text text-anchor=\"start\" x=\"861.5\" y=\"-394.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 1.0</text>\n",
       "<text text-anchor=\"start\" x=\"848\" y=\"-379.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 27126</text>\n",
       "<text text-anchor=\"start\" x=\"813\" y=\"-364.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [13563.0, 13563.0]</text>\n",
       "<text text-anchor=\"start\" x=\"870.5\" y=\"-349.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#eca572\" stroke=\"black\" d=\"M787,-306C787,-306 580,-306 580,-306 574,-306 568,-300 568,-294 568,-294 568,-235 568,-235 568,-229 574,-223 580,-223 580,-223 787,-223 787,-223 793,-223 799,-229 799,-235 799,-235 799,-294 799,-294 799,-300 793,-306 787,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"594\" y=\"-290.8\" font-family=\"helvetica\" font-size=\"14.00\">poutcome_success ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"624.5\" y=\"-275.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.766</text>\n",
       "<text text-anchor=\"start\" x=\"620\" y=\"-260.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 15298</text>\n",
       "<text text-anchor=\"start\" x=\"576\" y=\"-245.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [8333.333, 2395.002]</text>\n",
       "<text text-anchor=\"start\" x=\"646\" y=\"-230.8\" font-family=\"helvetica\" font-size=\"14.00\">class = no</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M832.4,-341.91C812.77,-331.83 791.61,-320.98 771.58,-310.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"772.96,-307.47 762.46,-306.02 769.76,-313.7 772.96,-307.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"770.12\" y=\"-326.12\" font-family=\"helvetica\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#96cbf1\" stroke=\"black\" d=\"M1265.5,-306C1265.5,-306 1049.5,-306 1049.5,-306 1043.5,-306 1037.5,-300 1037.5,-294 1037.5,-294 1037.5,-235 1037.5,-235 1037.5,-229 1043.5,-223 1049.5,-223 1049.5,-223 1265.5,-223 1265.5,-223 1271.5,-223 1277.5,-229 1277.5,-235 1277.5,-235 1277.5,-294 1277.5,-294 1277.5,-300 1271.5,-306 1265.5,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"1096.5\" y=\"-290.8\" font-family=\"helvetica\" font-size=\"14.00\">duration ≤ 472.5</text>\n",
       "<text text-anchor=\"start\" x=\"1098.5\" y=\"-275.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.903</text>\n",
       "<text text-anchor=\"start\" x=\"1094\" y=\"-260.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 11828</text>\n",
       "<text text-anchor=\"start\" x=\"1045.5\" y=\"-245.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [5229.667, 11167.998]</text>\n",
       "<text text-anchor=\"start\" x=\"1116.5\" y=\"-230.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M996.85,-341.91C1018.22,-331.74 1041.27,-320.78 1063.05,-310.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1064.78,-313.48 1072.3,-306.02 1061.77,-307.16 1064.78,-313.48\"/>\n",
       "<text text-anchor=\"middle\" x=\"1063.93\" y=\"-325.88\" font-family=\"helvetica\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#eb9c63\" stroke=\"black\" d=\"M450,-187C450,-187 243,-187 243,-187 237,-187 231,-181 231,-175 231,-175 231,-116 231,-116 231,-110 237,-104 243,-104 243,-104 450,-104 450,-104 456,-104 462,-110 462,-116 462,-116 462,-175 462,-175 462,-181 456,-187 450,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"290\" y=\"-171.8\" font-family=\"helvetica\" font-size=\"14.00\">duration ≤ 92.5</text>\n",
       "<text text-anchor=\"start\" x=\"287.5\" y=\"-156.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.671</text>\n",
       "<text text-anchor=\"start\" x=\"283\" y=\"-141.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 14982</text>\n",
       "<text text-anchor=\"start\" x=\"239\" y=\"-126.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [8237.827, 1757.202]</text>\n",
       "<text text-anchor=\"start\" x=\"309\" y=\"-111.8\" font-family=\"helvetica\" font-size=\"14.00\">class = no</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M568,-223.4C537,-212.64 503.34,-200.95 471.84,-190.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"472.73,-186.62 462.14,-186.65 470.44,-193.23 472.73,-186.62\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#57ace9\" stroke=\"black\" d=\"M773.5,-187C773.5,-187 593.5,-187 593.5,-187 587.5,-187 581.5,-181 581.5,-175 581.5,-175 581.5,-116 581.5,-116 581.5,-110 587.5,-104 593.5,-104 593.5,-104 773.5,-104 773.5,-104 779.5,-104 785.5,-110 785.5,-116 785.5,-116 785.5,-175 785.5,-175 785.5,-181 779.5,-187 773.5,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"622.5\" y=\"-171.8\" font-family=\"helvetica\" font-size=\"14.00\">duration ≤ 129.5</text>\n",
       "<text text-anchor=\"start\" x=\"624.5\" y=\"-156.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.558</text>\n",
       "<text text-anchor=\"start\" x=\"629\" y=\"-141.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 316</text>\n",
       "<text text-anchor=\"start\" x=\"589.5\" y=\"-126.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [95.506, 637.799]</text>\n",
       "<text text-anchor=\"start\" x=\"642.5\" y=\"-111.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M683.5,-222.91C683.5,-214.65 683.5,-205.86 683.5,-197.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"687,-197.02 683.5,-187.02 680,-197.02 687,-197.02\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e68641\" stroke=\"black\" d=\"M201,-68C201,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 201,0 201,0 207,0 213,-6 213,-12 213,-12 213,-56 213,-56 213,-62 207,-68 201,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"47.5\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.236</text>\n",
       "<text text-anchor=\"start\" x=\"47.5\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 5751</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [3233.08, 130.163]</text>\n",
       "<text text-anchor=\"start\" x=\"69\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = no</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M257.47,-103.88C234.73,-93.5 210.39,-82.4 188.04,-72.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.47,-69.01 178.92,-68.04 186.57,-75.38 189.47,-69.01\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#edaa79\" stroke=\"black\" d=\"M450,-68C450,-68 243,-68 243,-68 237,-68 231,-62 231,-56 231,-56 231,-12 231,-12 231,-6 237,0 243,0 243,0 450,0 450,0 456,0 462,-6 462,-12 462,-12 462,-56 462,-56 462,-62 456,-68 450,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"287.5\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.804</text>\n",
       "<text text-anchor=\"start\" x=\"287.5\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 9231</text>\n",
       "<text text-anchor=\"start\" x=\"239\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [5004.747, 1627.039]</text>\n",
       "<text text-anchor=\"start\" x=\"309\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = no</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346.5,-103.73C346.5,-95.52 346.5,-86.86 346.5,-78.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350,-78.3 346.5,-68.3 343,-78.3 350,-78.3\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#94caf1\" stroke=\"black\" d=\"M672.5,-68C672.5,-68 492.5,-68 492.5,-68 486.5,-68 480.5,-62 480.5,-56 480.5,-56 480.5,-12 480.5,-12 480.5,-6 486.5,0 492.5,0 492.5,0 672.5,0 672.5,0 678.5,0 684.5,-6 684.5,-12 684.5,-12 684.5,-56 684.5,-56 684.5,-62 678.5,-68 672.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"523.5\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.899</text>\n",
       "<text text-anchor=\"start\" x=\"528\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 127</text>\n",
       "<text text-anchor=\"start\" x=\"488.5\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [55.947, 121.486]</text>\n",
       "<text text-anchor=\"start\" x=\"541.5\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M645.89,-103.73C637.48,-94.61 628.55,-84.93 620.14,-75.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"622.57,-73.28 613.21,-68.3 617.42,-78.02 622.57,-73.28\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#48a5e7\" stroke=\"black\" d=\"M894.5,-68C894.5,-68 714.5,-68 714.5,-68 708.5,-68 702.5,-62 702.5,-56 702.5,-56 702.5,-12 702.5,-12 702.5,-6 708.5,0 714.5,0 714.5,0 894.5,0 894.5,0 900.5,0 906.5,-6 906.5,-12 906.5,-12 906.5,-56 906.5,-56 906.5,-62 900.5,-68 894.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"750\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.37</text>\n",
       "<text text-anchor=\"start\" x=\"750\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 189</text>\n",
       "<text text-anchor=\"start\" x=\"710.5\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [39.559, 516.314]</text>\n",
       "<text text-anchor=\"start\" x=\"763.5\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M728.56,-103.73C738.84,-94.42 749.75,-84.54 760.01,-75.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"762.64,-77.6 767.7,-68.3 757.94,-72.41 762.64,-77.6\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#dceefa\" stroke=\"black\" d=\"M1261,-187C1261,-187 1054,-187 1054,-187 1048,-187 1042,-181 1042,-175 1042,-175 1042,-116 1042,-116 1042,-110 1048,-104 1054,-104 1054,-104 1261,-104 1261,-104 1267,-104 1273,-110 1273,-116 1273,-116 1273,-175 1273,-175 1273,-181 1267,-187 1261,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"1073\" y=\"-171.8\" font-family=\"helvetica\" font-size=\"14.00\">contact_unknown ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"1098.5\" y=\"-156.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.993</text>\n",
       "<text text-anchor=\"start\" x=\"1098.5\" y=\"-141.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 8298</text>\n",
       "<text text-anchor=\"start\" x=\"1050\" y=\"-126.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [4049.686, 4911.489]</text>\n",
       "<text text-anchor=\"start\" x=\"1116.5\" y=\"-111.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1157.5,-222.91C1157.5,-214.65 1157.5,-205.86 1157.5,-197.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1161,-197.02 1157.5,-187.02 1154,-197.02 1161,-197.02\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#5eafea\" stroke=\"black\" d=\"M1628,-187C1628,-187 1421,-187 1421,-187 1415,-187 1409,-181 1409,-175 1409,-175 1409,-116 1409,-116 1409,-110 1415,-104 1421,-104 1421,-104 1628,-104 1628,-104 1634,-104 1640,-110 1640,-116 1640,-116 1640,-175 1640,-175 1640,-181 1634,-187 1628,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"1463.5\" y=\"-171.8\" font-family=\"helvetica\" font-size=\"14.00\">duration ≤ 657.5</text>\n",
       "<text text-anchor=\"start\" x=\"1465.5\" y=\"-156.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.631</text>\n",
       "<text text-anchor=\"start\" x=\"1465.5\" y=\"-141.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 3530</text>\n",
       "<text text-anchor=\"start\" x=\"1417\" y=\"-126.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [1179.981, 6256.509]</text>\n",
       "<text text-anchor=\"start\" x=\"1483.5\" y=\"-111.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1277.67,-225.19C1316.5,-212.81 1359.65,-199.06 1398.86,-186.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1400.28,-189.77 1408.74,-183.4 1398.15,-183.11 1400.28,-189.77\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#aad5f4\" stroke=\"black\" d=\"M1143,-68C1143,-68 936,-68 936,-68 930,-68 924,-62 924,-56 924,-56 924,-12 924,-12 924,-6 930,0 936,0 936,0 1143,0 1143,0 1149,0 1155,-6 1155,-12 1155,-12 1155,-56 1155,-56 1155,-62 1149,-68 1143,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"980.5\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.945</text>\n",
       "<text text-anchor=\"start\" x=\"980.5\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 5934</text>\n",
       "<text text-anchor=\"start\" x=\"932\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [2729.554, 4790.004]</text>\n",
       "<text text-anchor=\"start\" x=\"998.5\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1113.56,-103.73C1103.54,-94.42 1092.89,-84.54 1082.89,-75.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1085.1,-72.54 1075.38,-68.3 1080.33,-77.67 1085.1,-72.54\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#e78d4b\" stroke=\"black\" d=\"M1383.5,-68C1383.5,-68 1185.5,-68 1185.5,-68 1179.5,-68 1173.5,-62 1173.5,-56 1173.5,-56 1173.5,-12 1173.5,-12 1173.5,-6 1179.5,0 1185.5,0 1185.5,0 1383.5,0 1383.5,0 1389.5,0 1395.5,-6 1395.5,-12 1395.5,-12 1395.5,-56 1395.5,-56 1395.5,-62 1389.5,-68 1383.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"1225.5\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.417</text>\n",
       "<text text-anchor=\"start\" x=\"1225.5\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 2364</text>\n",
       "<text text-anchor=\"start\" x=\"1181.5\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [1320.132, 121.486]</text>\n",
       "<text text-anchor=\"start\" x=\"1247\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = no</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1204.79,-103.73C1215.69,-94.33 1227.26,-84.35 1238.12,-74.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1240.59,-77.48 1245.88,-68.3 1236.02,-72.18 1240.59,-77.48\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#79bded\" stroke=\"black\" d=\"M1623.5,-68C1623.5,-68 1425.5,-68 1425.5,-68 1419.5,-68 1413.5,-62 1413.5,-56 1413.5,-56 1413.5,-12 1413.5,-12 1413.5,-6 1419.5,0 1425.5,0 1425.5,0 1623.5,0 1623.5,0 1629.5,0 1635.5,-6 1635.5,-12 1635.5,-12 1635.5,-56 1635.5,-56 1635.5,-62 1629.5,-68 1623.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"1465.5\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.804</text>\n",
       "<text text-anchor=\"start\" x=\"1465.5\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 1696</text>\n",
       "<text text-anchor=\"start\" x=\"1421.5\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [684.366, 2104.304]</text>\n",
       "<text text-anchor=\"start\" x=\"1483.5\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1524.5,-103.73C1524.5,-95.52 1524.5,-86.86 1524.5,-78.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1528,-78.3 1524.5,-68.3 1521,-78.3 1528,-78.3\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#51a9e8\" stroke=\"black\" d=\"M1863.5,-68C1863.5,-68 1665.5,-68 1665.5,-68 1659.5,-68 1653.5,-62 1653.5,-56 1653.5,-56 1653.5,-12 1653.5,-12 1653.5,-6 1659.5,0 1665.5,0 1665.5,0 1863.5,0 1863.5,0 1869.5,0 1875.5,-6 1875.5,-12 1875.5,-12 1875.5,-56 1875.5,-56 1875.5,-62 1869.5,-68 1863.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"1710\" y=\"-52.8\" font-family=\"helvetica\" font-size=\"14.00\">entropy = 0.49</text>\n",
       "<text text-anchor=\"start\" x=\"1705.5\" y=\"-37.8\" font-family=\"helvetica\" font-size=\"14.00\">samples = 1834</text>\n",
       "<text text-anchor=\"start\" x=\"1661.5\" y=\"-22.8\" font-family=\"helvetica\" font-size=\"14.00\">value = [495.615, 4152.204]</text>\n",
       "<text text-anchor=\"start\" x=\"1723.5\" y=\"-7.8\" font-family=\"helvetica\" font-size=\"14.00\">class = yes</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1613.53,-103.88C1636.27,-93.5 1660.61,-82.4 1682.96,-72.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1684.43,-75.38 1692.08,-68.04 1681.53,-69.01 1684.43,-75.38\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f6d55cae0b8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dtree = get_tree_2_plot(clf)\n",
    "Dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.618\n",
      "Validation f1 score: 0.629\n"
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf,train_X, train_y, val_X, val_y)\n",
    "print(\"Train f1 score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation f1 score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Did we achieve the desired target value? If not what do you thing the above results indicate: over-fitting, under-fitting\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Based on the answer to the above question, what do you think is the best course of action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> What are the hyper parameters of the `DecisionTreeClassifier`?\n",
    "\n",
    "You may decide to tune the important hyper-paramters of the decision tree classifier (identified in the above question) to get the best performance. As an example I have selected two hyper parameters:  `max_depth` and `min_samples_split`.\n",
    "\n",
    "In this exercise I will be using GridSearch to tune my parameters. Sklearn has a function that do cross validation to tune the hyper parameters called `GridSearchCV`. Lets use this function. \n",
    "\n",
    "*This step may take several steps depending on the performance of your computer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(class_weight='balanced',\n",
       "                                              criterion='entropy'),\n",
       "             param_grid={'max_depth': array([  2,  52, 102, 152, 202, 252, 302, 352]),\n",
       "                         'min_samples_split': array([ 2,  7, 12, 17, 22, 27, 32, 37, 42, 47])},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':np.arange(2,400, 50), 'min_samples_split':np.arange(2,50,5)}\n",
    "\n",
    "dt_clf = tree.DecisionTreeClassifier(criterion='entropy', class_weight='balanced')\n",
    "Gridclf = GridSearchCV(dt_clf, parameters, scoring='f1_macro')\n",
    "Gridclf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043358</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.555917</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.549189</td>\n",
       "      <td>0.608921</td>\n",
       "      <td>0.553087</td>\n",
       "      <td>0.558169</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041559</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 7}</td>\n",
       "      <td>0.555917</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.549189</td>\n",
       "      <td>0.608921</td>\n",
       "      <td>0.553087</td>\n",
       "      <td>0.558169</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041824</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 12}</td>\n",
       "      <td>0.555917</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.549189</td>\n",
       "      <td>0.608921</td>\n",
       "      <td>0.553087</td>\n",
       "      <td>0.558169</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042206</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 17}</td>\n",
       "      <td>0.555917</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.549189</td>\n",
       "      <td>0.608921</td>\n",
       "      <td>0.553087</td>\n",
       "      <td>0.558169</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041640</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 22}</td>\n",
       "      <td>0.555917</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.549189</td>\n",
       "      <td>0.608921</td>\n",
       "      <td>0.553087</td>\n",
       "      <td>0.558169</td>\n",
       "      <td>0.027832</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.211181</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>352</td>\n",
       "      <td>27</td>\n",
       "      <td>{'max_depth': 352, 'min_samples_split': 27}</td>\n",
       "      <td>0.716843</td>\n",
       "      <td>0.700609</td>\n",
       "      <td>0.704374</td>\n",
       "      <td>0.704933</td>\n",
       "      <td>0.713734</td>\n",
       "      <td>0.708099</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.210861</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>352</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 352, 'min_samples_split': 32}</td>\n",
       "      <td>0.713605</td>\n",
       "      <td>0.699537</td>\n",
       "      <td>0.706359</td>\n",
       "      <td>0.710326</td>\n",
       "      <td>0.716583</td>\n",
       "      <td>0.709282</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.206450</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>352</td>\n",
       "      <td>37</td>\n",
       "      <td>{'max_depth': 352, 'min_samples_split': 37}</td>\n",
       "      <td>0.708567</td>\n",
       "      <td>0.696393</td>\n",
       "      <td>0.710015</td>\n",
       "      <td>0.711235</td>\n",
       "      <td>0.715955</td>\n",
       "      <td>0.708433</td>\n",
       "      <td>0.006510</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.202843</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>352</td>\n",
       "      <td>42</td>\n",
       "      <td>{'max_depth': 352, 'min_samples_split': 42}</td>\n",
       "      <td>0.712779</td>\n",
       "      <td>0.700790</td>\n",
       "      <td>0.709354</td>\n",
       "      <td>0.713353</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.709655</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.202601</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>352</td>\n",
       "      <td>47</td>\n",
       "      <td>{'max_depth': 352, 'min_samples_split': 47}</td>\n",
       "      <td>0.715400</td>\n",
       "      <td>0.702184</td>\n",
       "      <td>0.711455</td>\n",
       "      <td>0.715364</td>\n",
       "      <td>0.710914</td>\n",
       "      <td>0.711064</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.043358      0.001812         0.003429        0.000072   \n",
       "1        0.041559      0.000310         0.003435        0.000079   \n",
       "2        0.041824      0.000441         0.003437        0.000097   \n",
       "3        0.042206      0.000442         0.003452        0.000052   \n",
       "4        0.041640      0.000458         0.003427        0.000109   \n",
       "..            ...           ...              ...             ...   \n",
       "75       0.211181      0.003677         0.003868        0.000101   \n",
       "76       0.210861      0.003592         0.003969        0.000197   \n",
       "77       0.206450      0.004447         0.003896        0.000124   \n",
       "78       0.202843      0.003651         0.003852        0.000123   \n",
       "79       0.202601      0.006531         0.003889        0.000076   \n",
       "\n",
       "   param_max_depth param_min_samples_split  \\\n",
       "0                2                       2   \n",
       "1                2                       7   \n",
       "2                2                      12   \n",
       "3                2                      17   \n",
       "4                2                      22   \n",
       "..             ...                     ...   \n",
       "75             352                      27   \n",
       "76             352                      32   \n",
       "77             352                      37   \n",
       "78             352                      42   \n",
       "79             352                      47   \n",
       "\n",
       "                                         params  split0_test_score  \\\n",
       "0      {'max_depth': 2, 'min_samples_split': 2}           0.555917   \n",
       "1      {'max_depth': 2, 'min_samples_split': 7}           0.555917   \n",
       "2     {'max_depth': 2, 'min_samples_split': 12}           0.555917   \n",
       "3     {'max_depth': 2, 'min_samples_split': 17}           0.555917   \n",
       "4     {'max_depth': 2, 'min_samples_split': 22}           0.555917   \n",
       "..                                          ...                ...   \n",
       "75  {'max_depth': 352, 'min_samples_split': 27}           0.716843   \n",
       "76  {'max_depth': 352, 'min_samples_split': 32}           0.713605   \n",
       "77  {'max_depth': 352, 'min_samples_split': 37}           0.708567   \n",
       "78  {'max_depth': 352, 'min_samples_split': 42}           0.712779   \n",
       "79  {'max_depth': 352, 'min_samples_split': 47}           0.715400   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.523732           0.549189           0.608921   \n",
       "1            0.523732           0.549189           0.608921   \n",
       "2            0.523732           0.549189           0.608921   \n",
       "3            0.523732           0.549189           0.608921   \n",
       "4            0.523732           0.549189           0.608921   \n",
       "..                ...                ...                ...   \n",
       "75           0.700609           0.704374           0.704933   \n",
       "76           0.699537           0.706359           0.710326   \n",
       "77           0.696393           0.710015           0.711235   \n",
       "78           0.700790           0.709354           0.713353   \n",
       "79           0.702184           0.711455           0.715364   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.553087         0.558169        0.027832               71  \n",
       "1            0.553087         0.558169        0.027832               71  \n",
       "2            0.553087         0.558169        0.027832               71  \n",
       "3            0.553087         0.558169        0.027832               71  \n",
       "4            0.553087         0.558169        0.027832               71  \n",
       "..                ...              ...             ...              ...  \n",
       "75           0.713734         0.708099        0.006135               48  \n",
       "76           0.716583         0.709282        0.005943               30  \n",
       "77           0.715955         0.708433        0.006510               47  \n",
       "78           0.712000         0.709655        0.004639               21  \n",
       "79           0.710914         0.711064        0.004823                5  \n",
       "\n",
       "[80 rows x 15 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Gridclf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7114556668878583\n",
      "{'max_depth': 352, 'min_samples_split': 22}\n"
     ]
    }
   ],
   "source": [
    "print(Gridclf.best_score_)\n",
    "print(Gridclf.best_params_)\n",
    "\n",
    "clf = Gridclf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 score: 0.840\n",
      "Validation f1 score: 0.722\n"
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf,train_X, train_y, val_X, val_y)\n",
    "print(\"Train f1 score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation f1 score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Did we achieve the desired target value? If not what do you thing the above results indicate: over-fitting, under-fitting\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> Based on the answer to the above question, what do you think is the best course of action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning decision trees with cost complexity pruning\n",
    "\n",
    "\n",
    "The DecisionTreeClassifier provides parameters such as min_samples_leaf and max_depth to prevent a tree from overfitting. Those parameters prevent the tree from growing to large size and are examples of pre pruning. \n",
    "\n",
    "Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting. This algorithm finds the node with the ''weakest link'' characterised by an effective alpha. Then the nodes with the smallest effective alpha are pruned first. as the algorithm works after the tree is grown, this is a post pruning technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(class_weight='balanced')\n",
    "path = clf.cost_complexity_pruning_path(train_X, train_y)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following step may take several steps depending on the performance of your computer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha, class_weight='balanced')\n",
    "    clf.fit(train_X, train_y)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = [f1_score(train_y, clf.predict(train_X), average='macro') for clf in clfs]\n",
    "val_scores = [f1_score(val_y, clf.predict(val_X), average='macro') for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"f1_score\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, val_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> What `ccp_alphas` value would you choose as the best for the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "Lets make many trees using our dataset. If we run the DT algorithm multiple times on same data, it will result in the same tree. To make different trees we can inject some randomness. Select data data points and features to be used in DT algorithm randomly - this process is called creating boor strapped datasets.\n",
    "\n",
    "This is automatically done in sklearn for us in the `RandomForestClassifier`. Lets use that on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=8, n_estimators=500, class_weight='balanced_subsample', random_state=0)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_acc_scores(clf, train_X, train_y, val_X, val_y)\n",
    "print(\"Train Accuracy score: {:.3f}\".format(train_acc))\n",
    "print(\"Validation Accuracy score: {:.3f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<font color='red'><span style=\"font-size:1.5em;\">☞</span> Task: Now tune the hyper parameters of the random forest.</font>**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Is the final model that you get after hyper parameter tuning better than the previous decision tree model? Why?\n",
    "\n",
    "Lets visualise the feature importance of the RF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = clf.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(dataXExpand.columns[sorted_idx], tree_feature_importances[sorted_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> Based on the above figure, do you see any reason to be concerned about the model?\n",
    "\n",
    "<span style=\"font-size:1.5em;\">�</span> If the model uses `duration` to predict the target, what can be an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Regression Decision Tree\n",
    "\n",
    "A regression decision tree can also be trained.  These are decision trees where the leaf node is a regression function. You will investigate learning regression trees using the boston housing data set from previous labs.\n",
    "\n",
    "The below code snippet will help get you started. Note that it does not make sense to use entropy for generating splits, so the default method from sklearn will be used. Also note that the DecisionTreeRegressor class uses similar pre-pruning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import sklearn \n",
    "\n",
    "# from sklearn import tree\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import metrics\n",
    "# from sklearn import model_selection\n",
    "\n",
    "# Load data\n",
    "\n",
    "# bostonDataTarget = bostonData['MEDV']\n",
    "# bostonDataAttrs = bostonData.drop(columns='MEDV')\n",
    "# trainY, testY, trainX, testX = model_selection.train_test_split(np.array(bostonDataTarget),np.array(bostonDataAttrs), test_size=0.2)\n",
    "# clfBoston = sklearn.tree.DecisionTreeRegressor(max_depth=5, min_samples_split=5)\n",
    "# clfBoston = clfBoston.fit(trainX, trainY)\n",
    "# predictions = clfBoston.predict(testX)\n",
    "# metrics.mean_squared_error(testY, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.5em;\">�</span> How does the error on the regression decision tree compare to the best results you have found in previous labs?\n",
    "    \n",
    "<span style=\"font-size:1.5em;\">�</span> Find a good set of pre-pruning parameters that minimises the mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
